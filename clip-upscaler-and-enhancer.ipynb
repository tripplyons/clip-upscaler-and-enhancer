{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--C7jvPqrrSB"
      },
      "source": [
        "# CLIP Upscaler and Enhancer\n",
        "Using OpenAI's CLIP to upscale and enhance images\n",
        "\n",
        "[![GitHub Repo stars](https://img.shields.io/github/stars/tripplyons/clip-upscaler-and-enhancer?style=social)](https://github.com/tripplyons/clip-upscaler-and-enhancer)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripplyons/clip-upscaler-and-enhancer/blob/main/clip-upscaler-and-enhancer.ipynb)\n",
        "\n",
        "Based on [nshepperd's JAX CLIP Guided Diffusion v2.4](https://colab.research.google.com/drive/10YWuTxtBI7PS0xBJCLAUjhR5cB0UUXe-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZz-fFxNBCug"
      },
      "source": [
        "# Original Notebook Description (from nshepperd's JAX CLIP Guided Diffusion v2.4)\n",
        "\n",
        "## Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "Based on my previous jax port of Katherine Crowson's CLIP guided diffusion notebook.\n",
        " - [nshepperd's JAX CLIP Guided Diffusion 512x512.ipynb](https://colab.research.google.com/drive/1ZZi1djM8lU4sorkve3bD6EBHiHs6uNAi)\n",
        " - [CLIP Guided Diffusion HQ 512x512.ipynb](https://colab.research.google.com/drive/1V66mUeJbXrTuQITvJunvnWVn96FEbSI3)\n",
        "\n",
        "Added multi-perceptor and pytree ~trickery~ while eliminating the complicated OpenAI gaussian_diffusion classes. Supports both 256x256 and 512x512 OpenAI models (just change the `'image_size': 256` under Model Settings).\n",
        " - Added small secondary model for clip guidance.\n",
        " - Added anti-jpeg model for clearer samples.\n",
        " - Added secondary anti-jpeg classifier.\n",
        " - Added Katherine Crowso's v diffusion models (<https://github.com/crowsonkb/v-diffusion-jax>).\n",
        " - Added pixel art model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zKX4uWFBks2"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson; nshepperd\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GLI2tEr0AUQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Mount drive for saving samples and caching model parameters\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_location = '/content/drive/MyDrive/samples/v2'\n",
        "model_location = '/content/drive/MyDrive/models'\n",
        "\n",
        "os.makedirs(save_location, exist_ok=True)\n",
        "os.makedirs(model_location, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKZYWJt087dj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv_myK0Hrb0G"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ1HpnuQHOpU"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi | grep A100 && pip install https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.71+cuda111-cp37-none-manylinux2010_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZS4uQYE9BXf"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install dm-haiku cbor2 ftfy einops\n",
        "!git clone https://github.com/kingoflolz/CLIP_JAX\n",
        "!git clone https://github.com/nshepperd/jax-guided-diffusion -b v2\n",
        "!git clone https://github.com/crowsonkb/v-diffusion-jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erek6UjoqR7O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./CLIP_JAX')\n",
        "sys.path.append('./jax-guided-diffusion')\n",
        "sys.path.append('./v-diffusion-jax')\n",
        "\n",
        "import math\n",
        "import io\n",
        "import time\n",
        "import functools\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import weakref\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "import jaxtorch\n",
        "from jaxtorch import PRNG, Context, Module, nn, init\n",
        "from tqdm import tqdm\n",
        "\n",
        "import clip_jax\n",
        "import diffusion as v_diffusion\n",
        "\n",
        "from lib.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from lib import util\n",
        "\n",
        "from IPython import display\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch.utils.data\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTWBAsCCqtIz"
      },
      "outputs": [],
      "source": [
        "devices = jax.devices()\n",
        "n_devices = len(devices)\n",
        "print('Using device:', devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVrjUvv0N-9K"
      },
      "outputs": [],
      "source": [
        "# Implement lazy loading and caching of model parameters for all the different models.\n",
        "\n",
        "class WeakKey(object):\n",
        "  \"\"\"Weak pointer equality based keys for hashable dicts. Does not keep x alive.\"\"\"\n",
        "  def __init__(self, x):\n",
        "    self.id = id(x)\n",
        "    self.weak = weakref.ref(x)\n",
        "  def __hash__(self):\n",
        "    return hash(self.id)\n",
        "  def __eq__(self, other):\n",
        "    a = self.weak()\n",
        "    b = other.weak()\n",
        "    return self.id == other.id and (a is b)\n",
        "\n",
        "class WeakCache(object):\n",
        "  \"\"\"A cache using weak references so values are cached only as long as they are referenced from elsewhere.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.cache = {}\n",
        "\n",
        "  def lookup(self, f, x):\n",
        "    \"\"\"Look up the cached value of f(x).\"\"\"\n",
        "    key = WeakKey(x)\n",
        "    if key in self.cache:\n",
        "      val = self.cache[key]()\n",
        "      if val is not None:\n",
        "        return val\n",
        "    val = f(x)\n",
        "    self.cache[key] = weakref.ref(val)\n",
        "    return val\n",
        "\n",
        "gpu_cache = WeakCache()\n",
        "\n",
        "def to_gpu(params):\n",
        "  \"\"\"Convert a pytree of params to jax, using cached arrays if they are still alive.\"\"\"\n",
        "  return jax.tree_util.tree_map(lambda x: gpu_cache.lookup(jnp.array,x) if type(x) is np.ndarray else x, params)\n",
        "\n",
        "# @jax.tree_util.register_pytree_node_class\n",
        "class LazyParams(object):\n",
        "  \"\"\"Lazily download parameters and load onto gpu. Parameters are kept in cpu memory and only loaded to gpu as long as needed.\"\"\"\n",
        "  def __init__(self, load):\n",
        "    self.load = load\n",
        "    self.params = None\n",
        "  @staticmethod\n",
        "  def pt(url, key=None):\n",
        "    def load():\n",
        "      params = jaxtorch.pt.load(fetch_model(url))\n",
        "      if key is not None:\n",
        "        return params[key]\n",
        "      else:\n",
        "        return params\n",
        "    return LazyParams(load)\n",
        "  def __call__(self):\n",
        "    if self.params is None:\n",
        "      self.params = jax.tree_util.tree_map(np.array, self.load())\n",
        "    return to_gpu(self.params)\n",
        "\n",
        "  def tree_flatten(self):\n",
        "      return [self()], []\n",
        "  def tree_unflatten(static, dynamic):\n",
        "      return dynamic[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz_wPYY3qbax"
      },
      "outputs": [],
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    local_path = os.path.join(model_location, basename)\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    else:\n",
        "        !curl '{url_or_path}' -o '{local_path}'\n",
        "        return local_path\n",
        "\n",
        "\n",
        "def grey(image):\n",
        "    [*_, c, h, w] = image.shape\n",
        "    return jnp.broadcast_to(image.mean(axis=-3, keepdims=True), image.shape)\n",
        "\n",
        "def cutout_image(image, offsetx, offsety, size, output_size=224):\n",
        "    \"\"\"Computes (square) cutouts of an image given x and y offsets and size.\"\"\"\n",
        "    (c, h, w) = image.shape\n",
        "\n",
        "    scale = jnp.stack([output_size / size, output_size / size])\n",
        "    translation = jnp.stack([-offsety * output_size / size, -offsetx * output_size / size])\n",
        "    return jax.image.scale_and_translate(image,\n",
        "                                         shape=(c, output_size, output_size),\n",
        "                                         spatial_dims=(1,2),\n",
        "                                         scale=scale,\n",
        "                                         translation=translation,\n",
        "                                         method='lanczos3')\n",
        "\n",
        "def cutouts_images(image, offsetx, offsety, size, output_size=224):\n",
        "    f = partial(cutout_image, output_size=output_size)         # [c h w] [] [] [] -> [c h w]\n",
        "    f = jax.vmap(f, in_axes=(0, None, None, None), out_axes=0) # [n c h w] [] [] [] -> [n c h w]\n",
        "    f = jax.vmap(f, in_axes=(None, 0, 0, 0), out_axes=0)       # [n c h w] [k] [k] [k] -> [k n c h w]\n",
        "    return f(image, offsetx, offsety, size)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutouts(object):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1., p_grey=0.2, p_mixgrey=0.0):\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.p_grey = p_grey\n",
        "        self.p_mixgrey = p_mixgrey\n",
        "\n",
        "    def __call__(self, input, key):\n",
        "        [b, c, h, w] = input.shape\n",
        "        rng = PRNG(key)\n",
        "        max_size = min(h, w)\n",
        "        min_size = min(h, w, self.cut_size)\n",
        "        cut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])**self.cut_pow\n",
        "        sizes = (min_size + cut_us * (max_size - min_size + 1)).astype(jnp.int32).clamp(min_size, max_size)\n",
        "        offsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=w - sizes)\n",
        "        offsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=0, maxval=h - sizes)\n",
        "        cutouts = cutouts_images(input, offsets_x, offsets_y, sizes)\n",
        "\n",
        "        B1 = 40\n",
        "        B2 = 40\n",
        "        lcut_us = jax.random.uniform(rng.split(), shape=[self.cutn//2])\n",
        "        border = B1 + lcut_us * B2\n",
        "        lsizes = (max(h,w) + border).astype(jnp.int32)\n",
        "        loffsets_x = jax.random.uniform(rng.split(), [self.cutn//2], minval=w/2-lsizes/2-border, maxval=w/2-lsizes/2+border)\n",
        "        loffsets_y = jax.random.uniform(rng.split(), [self.cutn//2], minval=h/2-lsizes/2-border, maxval=h/2-lsizes/2+border)\n",
        "        lcutouts = cutouts_images(input, loffsets_x, loffsets_y, lsizes)\n",
        "\n",
        "        cutouts = jnp.concatenate([cutouts, lcutouts], axis=0)\n",
        "\n",
        "        greyed = grey(cutouts)\n",
        "\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        grey_rs = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_mixgrey, grey_rs * greyed + (1 - grey_rs) * cutouts, cutouts)\n",
        "\n",
        "        grey_us = jax.random.uniform(rng.split(), shape=[self.cutn, b, 1, 1, 1])\n",
        "        cutouts = jnp.where(grey_us < self.p_grey, greyed, cutouts)\n",
        "        return cutouts\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.p_grey, self.cut_pow, self.p_mixgrey], (self.cut_size, self.cutn))\n",
        "\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        (cut_size, cutn) = static\n",
        "        (p_grey, cut_pow, p_mixgrey) = dynamic\n",
        "        return MakeCutouts(cut_size, cutn, cut_pow, p_grey, p_mixgrey)\n",
        "\n",
        "def Normalize(mean, std):\n",
        "    mean = jnp.array(mean).reshape(3,1,1)\n",
        "    std = jnp.array(std).reshape(3,1,1)\n",
        "    def forward(image):\n",
        "        return (image - mean) / std\n",
        "    return forward\n",
        "\n",
        "def norm1(x):\n",
        "    \"\"\"Normalize to the unit sphere.\"\"\"\n",
        "    return x / x.square().sum(axis=-1, keepdims=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = norm1(x)\n",
        "    y = norm1(y)\n",
        "    return (x - y).square().sum(axis=-1).sqrt().div(2).arcsin().square().mul(2)\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    x_diff = input[..., :, 1:] - input[..., :, :-1]\n",
        "    y_diff = input[..., 1:, :] - input[..., :-1, :]\n",
        "    return x_diff.square().mean([1,2,3]) + y_diff.square().mean([1,2,3])\n",
        "\n",
        "def downscale2d(image, f):\n",
        "  [c, n, h, w] = image.shape\n",
        "  return jax.image.resize(image, [c, n, h//f, w//f], method='cubic')\n",
        "\n",
        "def upscale2d(image, f):\n",
        "  [c, n, h, w] = image.shape\n",
        "  return jax.image.resize(image, [c, n, h*f, w*f], method='cubic')\n",
        "\n",
        "def gaussian_blur(image, sigma, radius):\n",
        "    if len(image.shape) == 4:\n",
        "      [n, c, h, w] = image.shape\n",
        "      return gaussian_blur(image.reshape([n*c,h,w]), sigma, radius).reshape(image.shape)\n",
        "    # image : [c, h, w]\n",
        "    kernel_size = radius * 2 + 1\n",
        "    kernel_1d = jsp.stats.norm.pdf(jnp.linspace(-radius / sigma, radius / sigma, kernel_size))\n",
        "    kernel = (kernel_1d[:, None] @ kernel_1d[None, :])[None]\n",
        "    kernel = kernel / jnp.sum(kernel)\n",
        "    return jsp.signal.convolve(image, kernel, 'same')\n",
        "\n",
        "@dataclass\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class DiffusionOutput:\n",
        "    v: torch.Tensor\n",
        "    pred: torch.Tensor\n",
        "    eps: torch.Tensor\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return [self.v, self.pred, self.eps], []\n",
        "\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        return cls(*dynamic)\n",
        "  \n",
        "# Noise schedule\n",
        "\n",
        "def alpha_sigma_to_t(alpha, sigma):\n",
        "    return jnp.arctan2(sigma, alpha) * 2 / math.pi\n",
        "\n",
        "def cosine_t_to_ddpm(t):\n",
        "    alpha, sigma = get_cosine_alphas_sigmas(t)\n",
        "    log_snr = jnp.log(alpha**2 / sigma**2)\n",
        "    return ((jnp.log1p(jnp.exp(-log_snr)) - 1e-4) / 10).clamp(0,1).sqrt()\n",
        "\n",
        "def get_ddpm_alphas_sigmas(t):\n",
        "    log_snrs = -jnp.expm1(1e-4 + 10 * t**2).log()\n",
        "    alphas_squared = jax.nn.sigmoid(log_snrs)\n",
        "    sigmas_squared = jax.nn.sigmoid(-log_snrs)\n",
        "    return alphas_squared.sqrt(), sigmas_squared.sqrt()\n",
        "\n",
        "def get_cosine_alphas_sigmas(t):\n",
        "    return jnp.cos(t * math.pi/2), jnp.sin(t * math.pi/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsLm7ElGc6nQ"
      },
      "outputs": [],
      "source": [
        "# Define combinators.\n",
        "\n",
        "# These (ab)use the jax pytree registration system to define parameterised\n",
        "# objects for doing various things, which are compatible with jax.jit.\n",
        "\n",
        "# For jit compatibility an object needs to act as a pytree, which means implementing two methods:\n",
        "#  - tree_flatten(self): returns two lists of the object's fields: \n",
        "#       1. 'dynamic' parameters: things which can be jax tensors, or other pytrees\n",
        "#       2. 'static' parameters: arbitrary python objects, will trigger recompilation when changed\n",
        "#  - tree_unflatten(static, dynamic): reconstitutes the object from its parts\n",
        "\n",
        "# With these tricks, you can simply define your cond_fn as an object, as is done\n",
        "# below, and pass it into the jitted sample step as a regular argument. JAX will\n",
        "# handle recompiling the jitted code whenever a control-flow affecting parameter\n",
        "# is changed (such as cut_batches).\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CosineModel(object):\n",
        "    def __init__(self, model, params, **kwargs):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "      self.kwargs = kwargs\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "        cx = Context(self.params, key).eval_mode_()\n",
        "        return self.model(cx, x, cosine_t.broadcast_to([n]), **self.kwargs)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return CosineModel(model, params, **kwargs)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class OpenaiModel(object):\n",
        "    def __init__(self, model, params):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cx = Context(self.params, key).eval_mode_()\n",
        "        openai_t = (t * 1000).broadcast_to([n])\n",
        "        eps = self.model(cx, x, openai_t)[:, :3, :, :]\n",
        "        pred = (x - eps * sigma) / alpha\n",
        "        v    = (eps - x * sigma) / alpha\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params] = dynamic\n",
        "        [model] = static\n",
        "        return OpenaiModel(model, params)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class Perceptor(object):\n",
        "    # Wraps a CLIP instance and its parameters.\n",
        "    def __init__(self, image_fn, text_fn, clip_params):\n",
        "        self.image_fn = image_fn\n",
        "        self.text_fn = text_fn\n",
        "        self.clip_params = clip_params\n",
        "    @jax.jit\n",
        "    def embed_cutouts(self, cutouts):\n",
        "        return norm1(self.image_fn(self.clip_params, cutouts))\n",
        "    def embed_text(self, text):\n",
        "        tokens = clip_jax.tokenize([text])\n",
        "        text_embed = self.text_fn(self.clip_params, tokens)\n",
        "        return norm1(text_embed.reshape(512))\n",
        "    def embed_texts(self, texts):\n",
        "        return jnp.stack([self.embed_text(t) for t in texts])\n",
        "    def tree_flatten(self):\n",
        "        return [self.clip_params], [self.image_fn, self.text_fn]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [clip_params] = dynamic\n",
        "        [image_fn, text_fn] = static\n",
        "        return Perceptor(image_fn, text_fn, clip_params)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class LerpModels(object):\n",
        "    \"\"\"Linear combination of diffusion models.\"\"\"\n",
        "    def __init__(self, models):\n",
        "        self.models = models\n",
        "    def __call__(self, x, t, key):\n",
        "        outputs = [m(x,t,key) for (m,w) in self.models]\n",
        "        v = sum(out.v * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        pred = sum(out.pred * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        eps = sum(out.eps * w for (out, (m,w)) in zip(outputs, self.models))\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.models], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return LerpModels(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class KatModel(object):\n",
        "    def __init__(self, model, params, **kwargs):\n",
        "      if isinstance(params, LazyParams):\n",
        "        params = params()\n",
        "      self.model = model\n",
        "      self.params = params\n",
        "      self.kwargs = kwargs\n",
        "    @jax.jit\n",
        "    def __call__(self, x, t, key):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma)\n",
        "        v = self.model.apply(self.params, key, x, cosine_t.broadcast_to([n]), self.kwargs)\n",
        "        pred = x * alpha - v * sigma\n",
        "        eps = x * sigma + v * alpha\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return KatModel(model, params, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnpzHXfMFpCI"
      },
      "outputs": [],
      "source": [
        "# Common nn modules.\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        return jnp.concatenate([self.main(cx, input), self.skip(cx, input)], axis=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = init.normal(out_features // 2, in_features, stddev=std)\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        f = 2 * math.pi * input @ cx[self.weight].transpose()\n",
        "        return jnp.concatenate([f.cos(), f.sin()], axis=-1)\n",
        "\n",
        "\n",
        "class AvgPool2d(nn.Module):\n",
        "    def forward(self, cx, x):\n",
        "        [n, c, h, w] = x.shape\n",
        "        x = x.reshape([n, c, h//2, 2, w//2, 2])\n",
        "        x = x.mean((3,5))\n",
        "        return x\n",
        "\n",
        "\n",
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].broadcast_to(list(input.shape) + [shape[2], shape[3]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlN36wo_TaFC"
      },
      "outputs": [],
      "source": [
        "# Secondary Model \n",
        "class ConvBlock(nn.Sequential):\n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "class SecondaryDiffusionImageNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, c),\n",
        "            ConvBlock(c, c),\n",
        "            SkipBlock([\n",
        "                AvgPool2d(),\n",
        "                # nn.image.Downsample2d('linear'),\n",
        "                ConvBlock(c, c * 2),\n",
        "                ConvBlock(c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    AvgPool2d(),\n",
        "                    # nn.image.Downsample2d('linear'),\n",
        "                    ConvBlock(c * 2, c * 4),\n",
        "                    ConvBlock(c * 4, c * 4),\n",
        "                    SkipBlock([\n",
        "                        AvgPool2d(),\n",
        "                        # nn.image.Downsample2d('linear'),\n",
        "                        ConvBlock(c * 4, c * 8),\n",
        "                        ConvBlock(c * 8, c * 4),\n",
        "                        nn.image.Upsample2d('linear'),\n",
        "                    ]),\n",
        "                    ConvBlock(c * 8, c * 4),\n",
        "                    ConvBlock(c * 4, c * 2),\n",
        "                    nn.image.Upsample2d('linear'),\n",
        "                ]),\n",
        "                ConvBlock(c * 4, c * 2),\n",
        "                ConvBlock(c * 2, c),\n",
        "                nn.image.Upsample2d('linear'),\n",
        "            ]),\n",
        "            ConvBlock(c * 2, c),\n",
        "            nn.Conv2d(c, 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "class SecondaryDiffusionImageNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16)\n",
        "        self.down = AvgPool2d()\n",
        "        self.up = nn.image.Upsample2d('linear')\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ConvBlock(3 + 16, cs[0]),\n",
        "            ConvBlock(cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                self.down,\n",
        "                ConvBlock(cs[0], cs[1]),\n",
        "                ConvBlock(cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    self.down,\n",
        "                    ConvBlock(cs[1], cs[2]),\n",
        "                    ConvBlock(cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        self.down,\n",
        "                        ConvBlock(cs[2], cs[3]),\n",
        "                        ConvBlock(cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            self.down,\n",
        "                            ConvBlock(cs[3], cs[4]),\n",
        "                            ConvBlock(cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                self.down,\n",
        "                                ConvBlock(cs[4], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[5]),\n",
        "                                ConvBlock(cs[5], cs[4]),\n",
        "                                self.up,\n",
        "                            ]),\n",
        "                            ConvBlock(cs[4] * 2, cs[4]),\n",
        "                            ConvBlock(cs[4], cs[3]),\n",
        "                            self.up,\n",
        "                        ]),\n",
        "                        ConvBlock(cs[3] * 2, cs[3]),\n",
        "                        ConvBlock(cs[3], cs[2]),\n",
        "                        self.up,\n",
        "                    ]),\n",
        "                    ConvBlock(cs[2] * 2, cs[2]),\n",
        "                    ConvBlock(cs[2], cs[1]),\n",
        "                    self.up,\n",
        "                ]),\n",
        "                ConvBlock(cs[1] * 2, cs[1]),\n",
        "                ConvBlock(cs[1], cs[0]),\n",
        "                self.up,\n",
        "            ]),\n",
        "            ConvBlock(cs[0] * 2, cs[0]),\n",
        "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "secondary1_model = SecondaryDiffusionImageNet()\n",
        "secondary1_params = secondary1_model.init_weights(jax.random.PRNGKey(0))\n",
        "secondary1_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet.pth')\n",
        "\n",
        "secondary2_model = SecondaryDiffusionImageNet2()\n",
        "secondary2_params = secondary2_model.init_weights(jax.random.PRNGKey(0))\n",
        "secondary2_params = LazyParams.pt('https://v-diffusion.s3.us-west-2.amazonaws.com/secondary_model_imagenet_2.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ThgwVPtXIH5"
      },
      "outputs": [],
      "source": [
        "# Anti-JPEG model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, cx, input):\n",
        "        return self.main(cx, input) + self.skip(cx, input)\n",
        "\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, dropout=True):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "        ], skip)\n",
        "        \n",
        "\n",
        "CHANNELS=64\n",
        "class JPEGModel(nn.Module):\n",
        "    def __init__(self, c=CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "        self.class_embed = nn.Embedding(3, 16)\n",
        "\n",
        "        self.arch = '11(22(22(2)22)22)11'\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3 + 16 + 16, c, 1),\n",
        "            ResConvBlock(c, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            SkipBlock([\n",
        "                nn.image.Downsample2d(),\n",
        "                ResConvBlock(c,     c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.image.Downsample2d(),\n",
        "                    ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                    ResConvBlock(c * 2, 2 * 2, c * 2),\n",
        "                    SkipBlock([\n",
        "                        nn.image.Downsample2d(),\n",
        "                        ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                        nn.image.Upsample2d(),\n",
        "                    ]),\n",
        "                    ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                    ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                    nn.image.Upsample2d(),\n",
        "                ]),\n",
        "                ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c),\n",
        "                nn.image.Upsample2d(),\n",
        "            ]),\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, ts, cond):\n",
        "        [n, c, h, w] = input.shape\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, ts[:, None]), input.shape)\n",
        "        class_embed = expand_to_planes(self.class_embed(cx, cond), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed, class_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(ts)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "jpeg_model = JPEGModel()\n",
        "jpeg_params = jpeg_model.init_weights(jax.random.PRNGKey(0))\n",
        "jpeg_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/jpeg-db-oi-614.pt', key='params_ema')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IQNdD-UXjw_"
      },
      "outputs": [],
      "source": [
        "# Secondary Anti-JPEG Classifier\n",
        "\n",
        "CHANNELS=64\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, c=CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "\n",
        "        self.arch = '11-22-22-22'\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3 + 16, c, 1),\n",
        "            ResConvBlock(c, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c,     c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, 2 * 2, c * 2),\n",
        "            nn.image.Downsample2d(),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, c * 2),\n",
        "            ResConvBlock(c * 2, c * 2, 1, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, ts):\n",
        "        [n, c, h, w] = input.shape\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, ts[:, None]), input.shape)\n",
        "        return self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "\n",
        "    def score(self, cx, reals, ts, cond, flood_level, blur_size):\n",
        "        cond = cond[:, None, None, None]\n",
        "        logits = self.forward(cx, reals, ts)\n",
        "        logits = gaussian_blur(logits, blur_size, 6)\n",
        "        loss = -jax.nn.log_sigmoid(jnp.where(cond==0, logits, -logits))\n",
        "        loss = loss.clamp(minval=flood_level, maxval=None)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def classifier_probs(classifier_params, x, ts):\n",
        "  n = x.shape[0]\n",
        "  cx = Context(classifier_params, jax.random.PRNGKey(0)).eval_mode_()\n",
        "  probs = jax.nn.sigmoid(classifier_model(cx, x, ts.broadcast_to([n])))\n",
        "  return probs\n",
        "\n",
        "classifier_model = Classifier()\n",
        "classifier_params = classifier_model.init_weights(jax.random.PRNGKey(0))\n",
        "classifier_params = LazyParams.pt('https://set.zlkj.in/models/diffusion/jpeg-classifier-72.pt', 'params_ema')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHqocqbM_NzV"
      },
      "outputs": [],
      "source": [
        "# Pixel art model\n",
        "\n",
        "CHANNELS=192\n",
        "class PixelArtV4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = CHANNELS  # The base channel count\n",
        "\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=1.0)\n",
        "\n",
        "        self.arch = '122222'\n",
        "\n",
        "        muls = [1, 2, 2, 2, 2, 2]\n",
        "        cs = [CHANNELS * m for m in muls]\n",
        "\n",
        "        def downsample(c1, c2):\n",
        "            return nn.Sequential(nn.image.Downsample2d(), nn.Conv2d(c1, c2, 1) if c1!=c2 else nn.Identity())\n",
        "\n",
        "        def upsample(c1, c2):\n",
        "            return nn.Sequential(nn.Conv2d(c1, c2, 1) if c1!=c2 else nn.Identity(), nn.image.Upsample2d())\n",
        "\n",
        "        class ResConvBlock(ResidualBlock):\n",
        "            def __init__(self, c_in, c_mid, c_out, dropout=True):\n",
        "                skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "                super().__init__([\n",
        "                    nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "                    nn.Dropout2d(p=0.1) if dropout else nn.Identity(),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "                    nn.Dropout2d(p=0.1) if dropout else nn.Identity(),\n",
        "                    nn.ReLU() if dropout else nn.Identity(),\n",
        "                ], skip)\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            ResConvBlock(3 + 16, cs[0], cs[0]),\n",
        "            ResConvBlock(cs[0], cs[0], cs[0]),\n",
        "            SkipBlock([\n",
        "                downsample(cs[0], cs[1]), # 2x2\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                SkipBlock([\n",
        "                    downsample(cs[1], cs[2]),  # 4x4\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    SkipBlock([\n",
        "                        downsample(cs[2], cs[3]),  # 8x8\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        SkipBlock([\n",
        "                            downsample(cs[3], cs[4]),  # 16x16\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            SkipBlock([\n",
        "                                downsample(cs[4], cs[5]),  # 32x32\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                ResConvBlock(cs[5], cs[5], cs[5]),\n",
        "                                upsample(cs[5],cs[4]),\n",
        "                            ]),\n",
        "                            ResConvBlock(cs[4]*2, cs[4], cs[4]),\n",
        "                            ResConvBlock(cs[4], cs[4], cs[4]),\n",
        "                            upsample(cs[4],cs[3]),\n",
        "                        ]),\n",
        "                        ResConvBlock(cs[3]*2, cs[3], cs[3]),\n",
        "                        ResConvBlock(cs[3], cs[3], cs[3]),\n",
        "                        upsample(cs[3],cs[2]),\n",
        "                    ]),\n",
        "                    ResConvBlock(cs[2]*2, cs[2], cs[2]),\n",
        "                    ResConvBlock(cs[2], cs[2], cs[2]),\n",
        "                    upsample(cs[2],cs[1]),\n",
        "                ]),\n",
        "                ResConvBlock(cs[1]*2, cs[1], cs[1]),\n",
        "                ResConvBlock(cs[1], cs[1], cs[1]),\n",
        "                upsample(cs[1],cs[0]),\n",
        "            ]),\n",
        "            ResConvBlock(cs[0]*2, cs[0], cs[0]),\n",
        "            ResConvBlock(cs[0], cs[0], 3, dropout=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, cx, input, t):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(cx, t[:, None]), input.shape)\n",
        "        v = self.net(cx, jnp.concatenate([input, timestep_embed], axis=1))\n",
        "        alphas, sigmas = get_cosine_alphas_sigmas(t)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        pred = input * alphas - v * sigmas\n",
        "        eps = input * sigmas + v * alphas\n",
        "        return DiffusionOutput(v, pred, eps)\n",
        "\n",
        "pixelartv4_model = PixelArtV4()\n",
        "pixelartv4_params = pixelartv4_model.init_weights(jax.random.PRNGKey(0))\n",
        "\n",
        "# There are many checkpoints supported with this model\n",
        "pixelartv4_params = LazyParams.pt(\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_34.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_63.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v4_150.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_50.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_65.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_97.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-v5_173.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_344.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_432.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_600.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_700.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_800.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_1000.pt'\n",
        "    # 'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_2000.pt'\n",
        "    'https://set.zlkj.in/models/diffusion/pixelart/pixelart-fgood_3000.pt',\n",
        "    key='params_ema'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e-9dz8u_3Ep"
      },
      "outputs": [],
      "source": [
        "@jax.tree_util.register_pytree_node_class\n",
        "class MakeCutoutsPixelated(object):\n",
        "    \"\"\"Used for pixel art model - nearest upscale by 4x before taking cutouts to present a more pixel-arty pred to CLIP.\"\"\"\n",
        "    def __init__(self, make_cutouts, factor=4):\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.factor = factor\n",
        "        self.cutn = make_cutouts.cutn\n",
        "    \n",
        "    def __call__(self, input, key):\n",
        "        [n, c, h, w] = input.shape\n",
        "        input = jax.image.resize(input, [n, c, h*self.factor, w * self.factor], method='nearest')\n",
        "        return self.make_cutouts(input, key)\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return ([self.make_cutouts], [self.factor])\n",
        "    @staticmethod\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MakeCutoutsPixelated(*dynamic, *static)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M67CgKc1fDd"
      },
      "outputs": [],
      "source": [
        "# Kat models\n",
        "\n",
        "danbooru_128_model = v_diffusion.get_model('danbooru_128')\n",
        "danbooru_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/danbooru_128.pkl')))\n",
        "\n",
        "wikiart_256_model = v_diffusion.get_model('wikiart_256')\n",
        "wikiart_256_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/wikiart_256.pkl')))\n",
        "\n",
        "wikiart_128_model = v_diffusion.get_model('wikiart_128')\n",
        "wikiart_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/wikiart_128.pkl')))\n",
        "\n",
        "imagenet_128_model = v_diffusion.get_model('imagenet_128')\n",
        "imagenet_128_params = LazyParams(lambda: v_diffusion.load_params(fetch_model('https://v-diffusion.s3.us-west-2.amazonaws.com/imagenet_128.pkl')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiorqmdkGQo1"
      },
      "source": [
        "Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ4ozP3aqhAV"
      },
      "outputs": [],
      "source": [
        "use_checkpoint = False # Set to True to save some memory\n",
        "\n",
        "model_urls = {\n",
        "    512: 'https://set.zlkj.in/models/diffusion/512x512_diffusion_uncond_finetune_008100.pt',\n",
        "    256: 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'\n",
        "}\n",
        "\n",
        "# Load models, both 256 and 512\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',\n",
        "    'image_size': 512,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': use_checkpoint \n",
        "})\n",
        "\n",
        "\n",
        "openai_512_model, _ = create_model_and_diffusion(**model_config)\n",
        "openai_512_params = openai_512_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_512_params = LazyParams.pt(model_urls[512])\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',\n",
        "    'image_size': 256,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "    'use_checkpoint': use_checkpoint \n",
        "})\n",
        "\n",
        "openai_256_model, _ = create_model_and_diffusion(**model_config)\n",
        "openai_256_params = openai_256_model.init_weights(jax.random.PRNGKey(0))\n",
        "openai_256_params = LazyParams.pt(model_urls[256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXQ2Di_LRM46"
      },
      "outputs": [],
      "source": [
        "# Losses and cond fn.\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondCLIP(object):\n",
        "    # CLIP guidance loss. Pushes the image toward a text prompt.\n",
        "    def __init__(self, text_embed, clip_guidance_scale, perceptor, make_cutouts, cut_batches):\n",
        "        self.text_embed = text_embed\n",
        "        self.clip_guidance_scale = clip_guidance_scale\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.cut_batches = cut_batches\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).reshape([self.make_cutouts.cutn, n, 512])\n",
        "            losses = spherical_dist_loss(image_embeds, self.text_embed).mean(0)\n",
        "            return losses.sum() * self.clip_guidance_scale\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "    def tree_flatten(self):\n",
        "        return [self.text_embed, self.clip_guidance_scale, self.perceptor, self.make_cutouts], [self.cut_batches]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [text_embed, clip_guidance_scale, perceptor, make_cutouts] = dynamic\n",
        "        [cut_batches] = static\n",
        "        return CondCLIP(text_embed, clip_guidance_scale, perceptor, make_cutouts, cut_batches)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class InfoLOOB(object):\n",
        "    # CLIP guidance loss. Pushes the image toward a text prompt.\n",
        "    def __init__(self, text_embed, clip_guidance_scale, perceptor, make_cutouts, lm, cut_batches):\n",
        "        self.text_embed = text_embed\n",
        "        self.clip_guidance_scale = clip_guidance_scale\n",
        "        self.perceptor = perceptor\n",
        "        self.make_cutouts = make_cutouts\n",
        "        self.lm = lm\n",
        "        self.cut_batches = cut_batches\n",
        "    def __call__(self, x_in, key):\n",
        "        n = x_in.shape[0]\n",
        "        def main_clip_loss(x_in, key):\n",
        "            cutouts = normalize(self.make_cutouts(x_in.add(1).div(2), key)).rearrange('k n c h w -> (k n) c h w')\n",
        "            image_embeds = self.perceptor.embed_cutouts(cutouts).reshape([self.make_cutouts.cutn, n, 512])\n",
        "                  \n",
        "            all_image_embeds = norm1(image_embeds.mean(0))\n",
        "            all_text_embeds = norm1(self.text_embed)\n",
        "            sim_matrix = jnp.einsum('nc,mc->nm', all_image_embeds, all_text_embeds)\n",
        "\n",
        "            x = 1\n",
        "            xn = sim_matrix.shape[0]\n",
        "            def loob(sim_matrix):\n",
        "              diag = jnp.eye(xn) * sim_matrix\n",
        "              off_diag = (1 - jnp.eye(xn))*sim_matrix\n",
        "              return -diag.sum() + self.lm * off_diag.exp().sum(axis=-1).log().sum()\n",
        "            losses = (loob(sim_matrix) + loob(sim_matrix.transpose())) / x\n",
        "            return losses.sum() * self.clip_guidance_scale\n",
        "        num_cuts = self.cut_batches\n",
        "        keys = jnp.stack(jax.random.split(key, num_cuts))\n",
        "        main_clip_grad = jax.lax.scan(lambda total, key: (total + jax.grad(main_clip_loss)(x_in, key), key),\n",
        "                                        jnp.zeros_like(x_in),\n",
        "                                        keys)[0] / num_cuts\n",
        "        return main_clip_grad\n",
        "\n",
        "    def tree_flatten(self):\n",
        "        return [self.text_embed, self.clip_guidance_scale, self.perceptor, self.make_cutouts, self.lm], [self.cut_batches]\n",
        "    @classmethod\n",
        "    def tree_unflatten(cls, static, dynamic):\n",
        "        return cls(*dynamic, *static)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondTV(object):\n",
        "    # Multiscale Total Variation loss. Tries to smooth out the image.\n",
        "    def __init__(self, tv_scale):\n",
        "        self.tv_scale = tv_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def sum_tv_loss(x_in, f=None):\n",
        "            if f is not None:\n",
        "                x_in = downscale2d(x_in, f)\n",
        "            return tv_loss(x_in).sum() * self.tv_scale\n",
        "        tv_grad_512 = jax.grad(sum_tv_loss)(x_in)\n",
        "        tv_grad_256 = jax.grad(partial(sum_tv_loss,f=2))(x_in)\n",
        "        tv_grad_128 = jax.grad(partial(sum_tv_loss,f=4))(x_in)\n",
        "        return tv_grad_512 + tv_grad_256 + tv_grad_128        \n",
        "    def tree_flatten(self):\n",
        "        return [self.tv_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondTV(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondSat(object):\n",
        "    # Saturation loss. Tries to prevent the image from going out of range.\n",
        "    def __init__(self, sat_scale):\n",
        "        self.sat_scale = sat_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def saturation_loss(x_in):\n",
        "            return jnp.abs(x_in - x_in.clamp(minval=-1,maxval=1)).mean()\n",
        "        return self.sat_scale * jax.grad(saturation_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.sat_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondSat(*dynamic)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            return (x_in - self.target).square().mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return CondMSE(*dynamic)\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MaskedMSE(object):\n",
        "    # MSE loss. Targets the output towards an image.\n",
        "    def __init__(self, target, mse_scale, mask, grey=False):\n",
        "        self.target = target\n",
        "        self.mse_scale = mse_scale\n",
        "        self.mask = mask\n",
        "        self.grey = grey\n",
        "    def __call__(self, x_in, key):\n",
        "        def mse_loss(x_in):\n",
        "            if self.grey:\n",
        "              return (self.mask * grey(x_in - self.target).square()).mean()\n",
        "            else:\n",
        "              return (self.mask * (x_in - self.target).square()).mean()\n",
        "        return self.mse_scale * jax.grad(mse_loss)(x_in)\n",
        "    def tree_flatten(self):\n",
        "        return [self.target, self.mse_scale, self.mask], [self.grey]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MaskedMSE(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class MainCondFn(object):\n",
        "    # Used to construct the main cond_fn. Accepts a diffusion model which will \n",
        "    # be used for denoising, plus a list of 'conditions' which will\n",
        "    # generate gradient of a loss wrt the denoised, to be summed together.\n",
        "    def __init__(self, diffusion, conditions, use='pred'):\n",
        "        self.diffusion = diffusion\n",
        "        self.conditions = [c for c in conditions if c is not None]\n",
        "        self.use = use\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, key, x, t):\n",
        "        rng = PRNG(key)\n",
        "        n = x.shape[0]\n",
        "\n",
        "        alphas, sigmas = get_ddpm_alphas_sigmas(t)\n",
        "\n",
        "        def denoise(key, x):\n",
        "            pred = self.diffusion(x, t, key).pred\n",
        "            if self.use == 'pred':\n",
        "                return pred\n",
        "            elif self.use == 'x_in':\n",
        "                return pred * sigmas + x * alphas\n",
        "        (x_in, backward) = jax.vjp(partial(denoise, rng.split()), x)\n",
        "\n",
        "        total = jnp.zeros_like(x_in)\n",
        "        for cond in self.conditions:\n",
        "            total += cond(x_in, rng.split())\n",
        "        final_grad = -backward(total)[0]\n",
        "\n",
        "        # clamp gradients to a max of 0.2\n",
        "        magnitude = final_grad.square().mean(axis=(1,2,3), keepdims=True).sqrt()\n",
        "        final_grad = final_grad * jnp.where(magnitude > 0.2, 0.2 / magnitude, 1.0)\n",
        "        return final_grad  \n",
        "    def tree_flatten(self):\n",
        "        return [self.diffusion, self.conditions], [self.use]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        return MainCondFn(*dynamic, *static)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class ClassifierFn(object):\n",
        "    def __init__(self, model, params, guidance_scale, **kwargs):\n",
        "       self.model = model\n",
        "       self.params = params\n",
        "       self.guidance_scale = guidance_scale\n",
        "       self.kwargs = kwargs\n",
        "\n",
        "    @jax.jit\n",
        "    def __call__(self, key, x, t):\n",
        "        n = x.shape[0]\n",
        "        alpha, sigma = get_ddpm_alphas_sigmas(t)\n",
        "        cosine_t = alpha_sigma_to_t(alpha, sigma).broadcast_to([n])\n",
        "        def fwd(x):\n",
        "          cx = Context(self.params, key).eval_mode_()\n",
        "          return self.guidance_scale * self.model.score(cx, x, cosine_t, **self.kwargs)\n",
        "        return -jax.grad(fwd)(x)\n",
        "    def tree_flatten(self):\n",
        "        return [self.params, self.guidance_scale, self.kwargs], [self.model]\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [params, guidance_scale, kwargs] = dynamic\n",
        "        [model] = static\n",
        "        return ClassifierFn(model, params, guidance_scale, **kwargs)\n",
        "\n",
        "\n",
        "@jax.tree_util.register_pytree_node_class\n",
        "class CondFns(object):\n",
        "    def __init__(self, *conditions):\n",
        "        self.conditions = conditions\n",
        "    def __call__(self, key, x, t):\n",
        "        rng = PRNG(key)\n",
        "        total = jnp.zeros_like(x)\n",
        "        for cond in self.conditions:\n",
        "          total += cond(rng.split(), x, t)\n",
        "        return total\n",
        "    def tree_flatten(self):\n",
        "        return [self.conditions], []\n",
        "    def tree_unflatten(static, dynamic):\n",
        "        [conditions] = dynamic\n",
        "        return MixCondFn(*conditions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBlHR4k2lfiB"
      },
      "outputs": [],
      "source": [
        "def sample_step(key, x, t1, t2, diffusion, cond_fn, eta):\n",
        "    rng = PRNG(key)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    alpha1, sigma1 = get_ddpm_alphas_sigmas(t1)\n",
        "    alpha2, sigma2 = get_ddpm_alphas_sigmas(t2)\n",
        "\n",
        "    # Run the model\n",
        "    out = diffusion(x, t1, rng.split())\n",
        "    eps = out.eps\n",
        "    pred0 = out.pred\n",
        "\n",
        "    # # Predict the denoised image\n",
        "    # pred0 = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Adjust eps with conditioning gradient\n",
        "    cond_score = cond_fn(rng.split(), x, t1)\n",
        "    eps = eps - sigma1 * cond_score\n",
        "\n",
        "    # Predict the denoised image with conditioning\n",
        "    pred = (x - eps * sigma1) / alpha1\n",
        "\n",
        "    # Negative eta allows more extreme levels of noise.\n",
        "    ddpm_sigma = (sigma2**2 / sigma1**2).sqrt() * (1 - alpha1**2 / alpha2**2).sqrt()\n",
        "    ddim_sigma = jnp.where(eta >= 0.0, \n",
        "                           eta * ddpm_sigma, # Normal: eta interpolates between ddim and ddpm\n",
        "                           -eta * sigma2)    # Extreme: eta interpolates between ddim and q_sample(pred)\n",
        "    adjusted_sigma = (sigma2**2 - ddim_sigma**2).sqrt()\n",
        "\n",
        "    # Recombine the predicted noise and predicted denoised image in the\n",
        "    # correct proportions for the next step\n",
        "    x = pred * alpha2 + eps * adjusted_sigma\n",
        "\n",
        "    # Add the correct amount of fresh noise\n",
        "    x += jax.random.normal(rng.split(), x.shape) * ddim_sigma\n",
        "    return x, pred0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz4DN5r-B5ek"
      },
      "source": [
        "Load CLIP Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjE7Z7i3Mkza"
      },
      "outputs": [],
      "source": [
        "clip_size = 224\n",
        "normalize = Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                      std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "image_fn, text_fn, clip_params, vit32_preprocess = clip_jax.load('ViT-B/32')\n",
        "vit32 = Perceptor(image_fn, text_fn, clip_params)\n",
        "vit32_embed = image_fn\n",
        "vit32_params = clip_params\n",
        "\n",
        "image_fn, text_fn, clip_params, vit16_preprocess = clip_jax.load('ViT-B/16')\n",
        "vit16 = Perceptor(image_fn, text_fn, clip_params)\n",
        "vit16_embed = image_fn\n",
        "vit16_params = clip_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI--Oe5ZrVph"
      },
      "source": [
        "# Settings and Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOynujCUCB4M"
      },
      "source": [
        "## Configuration for the run\n",
        "\n",
        "By default, it will look for an image at `/content/init.png`\n",
        "\n",
        "Using the default settings, it takes about 45 minutes on a P100 on Google Colab. To lower the quality, lower the value of `steps` or the dimensions of `image_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxGgJmRzq3Cs"
      },
      "outputs": [],
      "source": [
        "seed = None # if None, uses the current time in seconds.\n",
        "image_size = (1024, 1024)\n",
        "batch_size = 1\n",
        "n_batches = 1\n",
        "\n",
        "# not used for image generation, just the filename of the output image\n",
        "all_title = \"enhancer\"\n",
        "title = [all_title] * batch_size\n",
        "\n",
        "clip_guidance_scale = 1000 # Note: with two perceptors, effective guidance scale is ~2x because they are added together.\n",
        "tv_scale = 150 #150  # Smooths out the image\n",
        "sat_scale = 600 # Tries to prevent pixel values from going out of range\n",
        "cutn = 8        # Effective cutn is cut_batches * this\n",
        "cut_pow = 1.0   # Affects the size of cutouts. Larger cut_pow -> smaller cutouts (down to the min of 224x244)\n",
        "cut_batches = 4\n",
        "make_cutouts = MakeCutouts(clip_size, cutn, cut_pow=cut_pow, p_mixgrey=0.0)\n",
        "\n",
        "steps = 1000     # Number of steps for sampling. Generally, more = better.\n",
        "eta = 0.0       # 0.0: DDIM | 1.0: DDPM | -1.0: Extreme noise (q_sample)\n",
        "init_image = 'init.png'      # Diffusion will start with a mixture of this image with noise.\n",
        "starting_noise = 0.25  # Between 0 and 1. When using init image, generally 0.5-0.8 is good. Lower starting noise makes the result look more like the init.\n",
        "init_weight_mse = 0    # MSE loss between the output and the init makes the result look more like the init (should be between 0 and width*height*3). \n",
        "                       # (LPIPS... will be added later)\n",
        "\n",
        "\n",
        "init_pil = Image.open(init_image)\n",
        "vit32_embedding = vit32_embed(vit32_params, np.expand_dims(vit32_preprocess(init_pil), 0))\n",
        "vit16_embedding = vit16_embed(vit16_params, np.expand_dims(vit16_preprocess(init_pil), 0))\n",
        "\n",
        "\n",
        "# OpenAI used T=1000 to 0. We've just rescaled to between 1 and 0.\n",
        "schedule = jnp.linspace(starting_noise, 0, steps+1)\n",
        "\n",
        "if init_image is not None:\n",
        "    init_array = Image.open(fetch(init_image)).convert('RGB')\n",
        "    init_array = init_array.resize(image_size, Image.LANCZOS)\n",
        "    init_array = jnp.array(TF.to_tensor(init_array)).unsqueeze(0).mul(2).sub(1)\n",
        "else:\n",
        "    init_array = None\n",
        "\n",
        "def config():\n",
        "    # Configure models and load parameters onto gpu.\n",
        "    # We do this in a function to avoid leaking gpu memory.\n",
        "\n",
        "    # -- Openai with anti-jpeg --\n",
        "    openai = OpenaiModel(openai_512_model, openai_512_params())\n",
        "    secondary2 = CosineModel(secondary2_model, secondary2_params())\n",
        "    jpeg_0 = CosineModel(jpeg_model, jpeg_params(), cond=jnp.array([0]*batch_size)) # Clean class\n",
        "    jpeg_1 = CosineModel(jpeg_model, jpeg_params(), cond=jnp.array([2]*batch_size)) # Noisy class\n",
        "\n",
        "    jpeg_classifier_fn = ClassifierFn(classifier_model, classifier_params(), \n",
        "                                      guidance_scale=10000.0, # will generally depend on image size\n",
        "                                      cond=jnp.array([0]*batch_size), # Clean class\n",
        "                                      flood_level=0.7, # Prevent over-optimization\n",
        "                                      blur_size=3.0)\n",
        "\n",
        "    diffusion = LerpModels([(openai, 1.0),\n",
        "                            (jpeg_0, 1.0),\n",
        "                            (jpeg_1, -1.0)])\n",
        "    cond_model = secondary2\n",
        "\n",
        "    cond_fn = CondFns(MainCondFn(cond_model, [\n",
        "                        CondCLIP(vit32_embedding, clip_guidance_scale, vit32, make_cutouts, cut_batches),\n",
        "                        CondCLIP(vit16_embedding, clip_guidance_scale, vit16, make_cutouts, cut_batches),\n",
        "                        CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "                        CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "                        CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "                        ], use='pred'),\n",
        "                      jpeg_classifier_fn,\n",
        "                      )\n",
        "\n",
        "    # # -- v diffusion models --\n",
        "    # # Uncomment one of the four below.\n",
        "    # diffusion = KatModel(wikiart_256_model, wikiart_256_params())\n",
        "    # diffusion = KatModel(wikiart_128_model, wikiart_128_params())\n",
        "    # diffusion = KatModel(danbooru_128_model, danbooru_128_params())\n",
        "    # diffusion = KatModel(imagenet_128_model, imagenet_128_params())\n",
        "    # cond_model = diffusion\n",
        "\n",
        "    # cond_fn = MainCondFn(cond_model, [\n",
        "    #             CondCLIP(vit32.embed_texts(title), clip_guidance_scale, vit32, make_cutouts, cut_batches),\n",
        "    #             CondCLIP(vit16.embed_texts(title), clip_guidance_scale, vit16, make_cutouts, cut_batches),\n",
        "    #             CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "    #             CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "    #             CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "    #             ], use='pred')\n",
        "    \n",
        "    # # -- pixel art model --\n",
        "    # diffusion = CosineModel(pixelartv4_model, pixelartv4_params())\n",
        "    # cond_model = diffusion\n",
        "    # cond_fn = MainCondFn(cond_model, [\n",
        "    #             CondCLIP(vit32.embed_texts(title), clip_guidance_scale, vit32, MakeCutoutsPixelated(make_cutouts), cut_batches),\n",
        "    #             CondCLIP(vit16.embed_texts(title), clip_guidance_scale, vit16, MakeCutoutsPixelated(make_cutouts), cut_batches),\n",
        "    #             CondTV(tv_scale) if tv_scale > 0 else None,\n",
        "    #             CondMSE(init_array, init_weight_mse) if init_weight_mse > 0 else None,\n",
        "    #             CondSat(sat_scale) if sat_scale > 0 else None,\n",
        "    #             ], use='pred')\n",
        "\n",
        "    return diffusion, cond_fn\n",
        "\n",
        "diffusion, cond_fn = config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoIL7ayzq7kC"
      },
      "outputs": [],
      "source": [
        "# Actually do the run\n",
        "\n",
        "def sanitize(title):\n",
        "  return title[:100].replace('/', '_').replace('\\\\', '_')\n",
        "\n",
        "@torch.no_grad()\n",
        "def run():\n",
        "    if seed is None:\n",
        "        local_seed = int(time.time())\n",
        "    else:\n",
        "        local_seed = seed\n",
        "    print(f'Starting run with seed {local_seed}...')\n",
        "    rng = PRNG(jax.random.PRNGKey(local_seed))\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "        ts = schedule\n",
        "        alphas, sigmas = get_ddpm_alphas_sigmas(ts)\n",
        "        cosine_ts = alpha_sigma_to_t(alphas, sigmas)\n",
        "\n",
        "        x = sigmas[0] * jax.random.normal(rng.split(), [batch_size, 3, image_size[1], image_size[0]])\n",
        "\n",
        "        if init_array is not None:\n",
        "            x = x + alphas[0] * init_array\n",
        "\n",
        "        # Main loop\n",
        "        local_steps = schedule.shape[0] - 1\n",
        "        for j in tqdm(range(local_steps)):\n",
        "            if ts[j] != ts[j+1]:\n",
        "                # Skip steps where the ts are the same, to make it easier to\n",
        "                # make complicated schedules out of cat'ing linspaces.\n",
        "                x, pred = sample_step(rng.split(), x, ts[j], ts[j+1], diffusion, cond_fn, eta)\n",
        "            if j % 50 == 0 or j == local_steps - 1:\n",
        "                images = pred.add(1).div(2).clamp(0, 1)\n",
        "                images = torch.tensor(np.array(images))\n",
        "                display.display(TF.to_pil_image(utils.make_grid(images, 4).cpu()))\n",
        "\n",
        "        # Save samples\n",
        "        os.makedirs('samples/grid', exist_ok=True)\n",
        "        os.makedirs(f'{save_location}/grid', exist_ok=True)\n",
        "        TF.to_pil_image(utils.make_grid(images, 4).cpu()).save(f'samples/grid/{timestring}_{sanitize(all_title)}.png')\n",
        "        TF.to_pil_image(utils.make_grid(images, 4).cpu()).save(f'{save_location}/grid/{timestring}_{sanitize(all_title)}.png')\n",
        "\n",
        "        os.makedirs('samples/images', exist_ok=True)\n",
        "        os.makedirs(f'{save_location}/images', exist_ok=True)\n",
        "        for k in range(batch_size):\n",
        "            this_title = sanitize(title[k])\n",
        "            dname = f'samples/images/{timestring}_{k}_{this_title}.png'\n",
        "            pil_image = TF.to_pil_image(images[k])\n",
        "            pil_image.save(dname)\n",
        "            pil_image.save(f'{save_location}/images/{timestring}_{k}_{this_title}.png')\n",
        "\n",
        "try:\n",
        "  run()\n",
        "  success = True\n",
        "except:\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "  success = False\n",
        "assert success\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Lv_myK0Hrb0G"
      ],
      "name": "clip-upscaler-and-enhancer",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
